#include <fstream>
#include <iostream>

#include "network.cpp"

/**
 * Stores `network` into the configuration file at the path `config_filepath`
 * 
 * @param config_filepath filepath to store configuration
 * @param network Network object to store at the specified filepath
 */
void store_network_config(std::string config_filepath, CNet::Network& network) {
    using namespace std;
    using namespace Eigen;
    using namespace CNet;

    ofstream output_file(config_filepath);
    if(!output_file.is_open()) {
        throw runtime_error("Output file \"" + config_filepath + "\" could not be opened");
    }

    output_file << network.layer_count() << " ";
    output_file << ((network.loss_calculator()) ? network.loss_calculator()->name() : "X") << " "; 
    output_file << ((network.optimizer()) ? network.optimizer()->name() : "X") << " ";
    for(double param : network.optimizer_hyperparameters()) {
        output_file << param << " ";
    }
    output_file << "\n\n";

    for (int i = 0; i < network.layer_count(); i++) {
        Layer current_layer = network.layer_at(i);
        
        output_file << current_layer.name() << " " << current_layer.activation_function()->name() << " " << current_layer.input_dimension() << " " << current_layer.output_dimension() << "\n";
        output_file << current_layer.weight_matrix() << "\n\n";
        output_file << current_layer.bias_vector() << "\n\n";
    }
    
}


/**
 * Returns a std::vector of strings containing `input` split by spaces.
 * 
 * This method is like Python's split method, except any empty strings don't appear in the output.
 * 
 * @param input string to split
 * @return `input` split at spaces (output contains no empty strings)
 */
std::vector<std::string> split_by_spaces(const std::string& input) {
    using namespace std;

    vector<string> output;
    istringstream iss(input);
    string item;

    while(iss >> item) {
        if(item != " " && item != "\n") {
            output.push_back(item);
        }

    }

    return output;
}



/**
 * Returns the network whose configuration is specified in the file at `config_filepath`.
 * 
 * @param config_filepath file to get network configuration from. The file must have been created by the `store_network_config` method
 * @return Network built from the given specification file
 */
CNet::Network load_network_config(std::string config_filepath) {
    using namespace std;
    using namespace Eigen;
    using namespace CNet;

    ifstream input_file(config_filepath);
    if(!input_file.is_open()) {
        throw runtime_error("Input file \"" + config_filepath + "\" could not be opened");
    }

    /*
    Loading process:

    Initial load
        Stage 0: header (# layers, loss calculator, optimizer)

    Repeats until there are no layers left:
        Stage 1: Layer header (layer name, optimizer, input/output dimension) + reset weight matrix
        Stage 2: Weight matrix + reset bias vector
        Stage 3: Bias vector + create layer and load into output network
    */

    int stage = 0; //0=number of layers, 1=header, 2=weight matrix, 3=bias vector

    int n_layers = 0; //Should not be changed after reading the first line
    int current_layer = 0;

    string layer_name; //custom name for the layer
    string layer_activation_function_name; //identifier for activation function (not the function itself)

    int input_dim = 0; //layer-wise
    int current_row = 0; //current row of matrix or vector
    int output_dim = 0; //layer-wise

    MatrixXd weight_matrix;
    VectorXd bias_vector;

    Network constructed_network;

   
    string current_line;
    while(std::getline(input_file, current_line)) {
        if(current_line.length() <= 0) {
            continue;
        }

        if (!current_line.empty() && current_line.back() == '\r')
            current_line.pop_back();
        auto split_line = split_by_spaces(current_line);

        //Stage 0: get expected number of layers, loss calculator, and optimizer + hyperparameters
        if(stage == 0) {
            assert(split_line.size() >= 3 && "First line must have at least 3 space-separated entries");

            try {
                n_layers = stoi(current_line);
                if(n_layers < 0) {
                    throw runtime_error("number of layers cannot be negative");
                }
            }
            catch(invalid_argument& e) {
                throw runtime_error("number of layers must be an integer");
            }
            
            //load loss calc
            if(split_line[1] != "X") {
                shared_ptr<LossCalculator> loss_calc = make_loss_calculator(split_line[1]);
                constructed_network.set_loss_calculator(loss_calc);
                loss_calc.reset();
            }
            //load optimizer + hyperparams
            if(split_line[2] != "X") {
 
                    
                //get hyperparameters from split line, as strings
                vector<string> hyperparams_string(split_line.begin() + 3, split_line.end());
                
                //convert strings to doubles
                vector<double> hyperparams;
                for(string str : hyperparams_string) {
                    try {
                        hyperparams.push_back(stod(str));
                    }
                    catch(invalid_argument& e) {
                        throw runtime_error("hyperparameters for optimizer in save file must be numbers");
                    }
                }

                if(split_line[2] == "sgd") {
                    assert(hyperparams.size() == 3 && "SGD optimizer in save file must have 3 hyperparameters");
                    assert(hyperparams[0] > 0 && "save file SGD learning rate must be positive");
                    assert(hyperparams[1] >= 0 && "save file SGD momentum coefficient cannot be negative");
                    assert((int)hyperparams[2] > 0 && "save file SGD batch size must be positive, when cast to an integer");

                    shared_ptr<SGD> sgd = make_shared<SGD>(hyperparams[0], hyperparams[1], (int)hyperparams[2]);
                    constructed_network.set_optimizer(sgd);
                    sgd.reset();
                }
                
            }

            stage = 1;
        }
        //Stage 1: get number of inputs (columns of weight matrix), outputs (rows of weight matrix), and layer name
        else if(stage == 1) {

            //split line by space
            
            if(current_line.size() < 4) {
                throw runtime_error("Layer header line must have at least 4 items");
            }

            //make layer name
            for (int i = 0; i < (int)split_line.size() - 3; i++) {
                layer_name += split_line[i];
                if(i < (int)split_line.size() - 4) {
                    layer_name.push_back(' ');
                }
            }
            
            //layer activation + dimensions
            layer_activation_function_name = split_line[split_line.size() - 3];
            input_dim = stoi(split_line[split_line.size() - 2]);
            output_dim = stoi(split_line[split_line.size() - 1]);
            
            weight_matrix = MatrixXd(output_dim, input_dim);
            stage = 2;
        }
        //Stage 2: load the weight matrix
        else if(stage == 2) {
            assert(input_dim > 0);
            assert(output_dim > 0);

            assert((int)split_line.size() == input_dim);
            
            //ignore blank lines
            if(split_line.size() == 0) {
                continue;
            }
            
            //load current row of matrix
            try {
                for(int c = 0; c < input_dim; c++) {
                    weight_matrix(current_row, c) = stod(split_line[c]);
                }
            }
            catch(invalid_argument& e) {
                throw runtime_error("not a number (in matrix)");
            }

            //move to next row, check if all rows have been loaded
            current_row++;
            if(current_row > output_dim-1) {
                bias_vector = VectorXd(output_dim);
                current_row = 0;
                stage = 3;
            }
        }
        //Stage 3: load bias vector
        else if(stage == 3) {
            // cout << layer_name << " " << input_dim << " " << output_dim << endl;
            // cout << weight_matrix;

            assert((int)split_line.size() == 1);
            
            try {
                bias_vector(current_row) = stod(split_line[0]);
            }
            catch(invalid_argument& e) {
                throw runtime_error("Not a number (in vector)");
            }

            current_row++;
            if(current_row > output_dim-1) {

                //Make the layer
                shared_ptr<ActivationFunction> activation_ptr = make_activation_function(layer_activation_function_name);
                Layer new_layer = Layer(input_dim, output_dim, activation_ptr, layer_name);
                new_layer.set_weight_matrix(weight_matrix);
                new_layer.set_bias_vector(bias_vector);
                activation_ptr.reset();

                constructed_network += new_layer;

                input_dim = 0;
                output_dim = 0;
                current_row = 0;
                layer_activation_function_name = "";
                layer_name = "";

                stage = 1;
                current_layer++;

                // cout << new_layer << endl;
            }

            
            //prevent overloading
            if(current_layer >= n_layers) {
                break;
            }
        }


        // cout << current_line << endl;
    }

    // cout << n_layers << endl;
    return constructed_network;
}